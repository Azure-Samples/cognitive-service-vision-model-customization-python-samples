{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Cognitive Service Vision Image Composition using Python\n",
    "\n",
    "This is a tutorial about using Cognitive Service Vision Image Composition.\n",
    "\n",
    "Cognitive Service Vision Image Composition are a collection of tools to pre-process images before running product recognition, so that better product recognition and planogram compliance results can be achieved.\n",
    "\n",
    "Currently, image composition feature are available in **EastUS**, **West US2**, and **West Europe** regions.\n",
    "\n",
    "Please create a computer vision resource on Azure portal, in **EastUS**, **West US2**, or **West Europe** region, if you don't already have one. You can use [Multi-service resource](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Canomaly-detector%2Clanguage-service%2Ccomputer-vision%2Cwindows) as well. \n",
    "\n",
    "![check resources/create_cv_resource.png if pic does not show up](./resources/create_cv_resource.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the python samples package\n",
    "\n",
    "Install the sample code including utility code helping you use Python to run image composition in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cognitive-service-vision-product-recogntion-python-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from cognitive_service_vision_model_customization_python_samples import ResourceType\n",
    "from cognitive_service_vision_model_customization_python_samples.clients import ImageCompositionClient\n",
    "from cognitive_service_vision_model_customization_python_samples.models import ImageStitchingRequest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "Resource name and resource key are needed for accessing the service, which you can find here:\n",
    "\n",
    "![check resources/credentials.png if pic does not show up](./resources/credentials.png)\n",
    "\n",
    "If you are using a multi-service resource, the endpoint can be found here:\n",
    "\n",
    "![check resources/multi_service_endpoint.png if pic does not show up](./resources/multi_service_endpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource and key\n",
    "resource_type = ResourceType.SINGLE_SERVICE_RESOURCE # or ResourceType.MULTI_SERVICE_RESOURCE\n",
    "\n",
    "resource_name = None\n",
    "multi_service_endpoint = None\n",
    "\n",
    "if resource_type == ResourceType.SINGLE_SERVICE_RESOURCE:\n",
    "    resource_name = '{specify_your_resource_name}'\n",
    "    assert resource_name\n",
    "else:\n",
    "    multi_service_endpoint = '{specify_your_service_endpoint}'\n",
    "    assert multi_service_endpoint\n",
    "\n",
    "resource_key = '{specify_your_resource_key}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run image stitching\n",
    "\n",
    "When your shelf is very long that a single camera shot cannot capture the entire length/height of the shelf, use image stitching tool to stitching all of them into one.\n",
    "\n",
    "### Prepare image urls for running image stitching\n",
    "\n",
    "Image stitching tool require a list of accessible image urls as input to stitch the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    \"https://example.com/image1.jpg\",\n",
    "    \"https://example.com/image2.jpg\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload local images to Azure Storage Blob Container and get SAS URLs (Optional)\n",
    "\n",
    "If you have images locally, consider putting them into a Azure Storage Account Blob Container and get the SAS url for each of the images.\n",
    "\n",
    "Below is the example code to upload images for stitching: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import BlobServiceClient, BlobSasPermissions, generate_blob_sas\n",
    "\n",
    "# Replace these values with your Azure Storage Account credentials\n",
    "connection_string = \"{your_connection_string}\"\n",
    "container_name = \"{your_container_name}\"\n",
    "\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Create a new container if it doesn't exist\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "container_client.create_container(exist_ok=True)\n",
    "\n",
    "# Upload images to the container\n",
    "image_directory = \"{path/to/your/images}\"\n",
    "image_files = os.listdir(image_directory)\n",
    "\n",
    "for image_file in image_files:\n",
    "    blob_client = container_client.get_blob_client(image_file)\n",
    "    with open(os.path.join(image_directory, image_file), \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"{image_file} uploaded to {container_name} container\")\n",
    "\n",
    "# Get list of SAS URLs\n",
    "image_urls = []\n",
    "expiry_time = datetime.utcnow() + timedelta(hours=1)\n",
    "\n",
    "for image_file in image_files:\n",
    "    blob_client = container_client.get_blob_client(image_file)\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_client.account_name,\n",
    "        container_name=blob_client.container_name,\n",
    "        blob_name=blob_client.blob_name,\n",
    "        snapshot=None,\n",
    "        account_key=blob_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=expiry_time\n",
    "    )\n",
    "\n",
    "    sas_url = f\"{blob_client.url}?{sas_token}\"\n",
    "    image_urls.append(sas_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run image stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ImageCompositionClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "stitching_request = ImageStitchingRequest(images=image_urls)\n",
    "response = client.stitch_images(request=stitching_request)\n",
    "\n",
    "# Display stitched image\n",
    "display(Image.open(io.BytesIO(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run image recitfication\n",
    "\n",
    "With image rectification, product recognition and planogram matching results will be better.\n",
    "\n",
    "The rectification control points can be easily selected using the GUI-based selection tool under `cognitive_service_vision_model_customization_python_samples/tools/select_rectification_control_points.py`.\n",
    "\n",
    "For the best result, you would want to select the four corners of the shelf in clock-wise order, the output from the selection tool can then be used as input for the image rectification example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognitive_service_vision_model_customization_python_samples.models import (\n",
    "    ImageRectificationRequest,\n",
    "    ImageRectificationControlPoints,\n",
    "    NormalizedCoordinate\n",
    ")\n",
    "from IPython.display import display\n",
    "\n",
    "# Accessible image url for image to be rectified, if you have images locally, can also follow the image upload steps above\n",
    "image_url = \"https://example.com/image1.jpg\"\n",
    "\n",
    "control_points = ImageRectificationControlPoints(\n",
    "    top_left=NormalizedCoordinate(x=0.1, y=0.1),\n",
    "    top_right=NormalizedCoordinate(x=0.9, y=0.1),\n",
    "    bottom_left=NormalizedCoordinate(x=0.1, y=0.9),\n",
    "    bottom_right=NormalizedCoordinate(x=0.9, y=0.9)\n",
    ")\n",
    "\n",
    "rectify_request = ImageRectificationRequest(url=image_url, control_points=control_points)\n",
    "response = client.rectify_image(request=rectify_request)\n",
    "display(Image.open(io.BytesIO(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "With the stitched and rectified image, you can now send the image for product recognition, please refer to the tutorial [here](./cognitive_service_vision_product_recognition.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
