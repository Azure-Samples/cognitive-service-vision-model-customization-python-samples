{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Cognitive Service Vision Product Recognition using Python\n",
    "\n",
    "This is a tutorial about using Cognitive Service Vision Product Recognition\n",
    "\n",
    "Currently, product recognition feature are available in **EastUS**, **West US2**, and **West Europe** regions.\n",
    "\n",
    "Please create a computer vision resource on Azure portal, in **EastUS**, **West US2**, or **West Europe** region, if you don't already have one. You can use [Multi-service resource](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Canomaly-detector%2Clanguage-service%2Ccomputer-vision%2Cwindows) as well. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the python samples package\n",
    "\n",
    "Install the sample code including utility code helping you use Python to run product recognition in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cognitive-service-vision-product-recogntion-python-samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "Resource name and resource key are needed for accessing the service, which you can find here:\n",
    "\n",
    "![check media/credentials.png if pic does not show up](./resources/credentials.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource and key\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from cognitive_service_vision_model_customization_python_samples import ResourceType\n",
    "\n",
    "resource_type = ResourceType.SINGLE_SERVICE_RESOURCE # or ResourceType.MULTI_SERVICE_RESOURCE\n",
    "\n",
    "resource_name = None\n",
    "multi_service_endpoint = None\n",
    "\n",
    "if resource_type == ResourceType.SINGLE_SERVICE_RESOURCE:\n",
    "    resource_name = '{specify_your_resource_name}'\n",
    "    assert resource_name\n",
    "else:\n",
    "    multi_service_endpoint = '{specify_your_service_endpoint}'\n",
    "    assert multi_service_endpoint\n",
    "\n",
    "resource_key = '{specify_your_resource_key}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process your image using the image composition tool (Optional)\n",
    "\n",
    "Follow the tutorial [here](./cognitive_service_vision_image_composition.ipynb) to stitch and rectify your images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run product recognition with pre-built model\n",
    "\n",
    "You can run product recognition to detect products and gaps using our prebuilt model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Product recognition running 520993ee-f64b-4311-b717-a61d3687e4b0 for 2.552434206008911 seconds. Status ProductRecognitionStatus.NOT_STARTED.\n",
      "INFO:cognitive_service_vision_model_customization_python_samples.clients.product_recognition_client:Product recognition finished with state ProductRecognitionStatus.SUCCEEDED.\n",
      "INFO:cognitive_service_vision_model_customization_python_samples.clients.product_recognition_client:Wall-clock time 0.04254057010014852 minutes.\n",
      "INFO:cognitive_service_vision_model_customization_python_samples.clients.product_recognition_client:Product recognition result: {'imageMetadata': {'width': 2586, 'height': 3161}, 'products': [{'id': '1', 'boundingBox': {'x': 408, 'y': 66, 'w': 431, 'h': 366}, 'tags': [{'name': 'product', 'confidence': 0.9768970608711243}]}, {'id': '2', 'boundingBox': {'x': 1734, 'y': 69, 'w': 437, 'h': 357}, 'tags': [{'name': 'product', 'confidence': 0.9728712439537048}]}, {'id': '3', 'boundingBox': {'x': 1408, 'y': 2153, 'w': 264, 'h': 386}, 'tags': [{'name': 'product', 'confidence': 0.9722458720207214}]}, {'id': '4', 'boundingBox': {'x': 319, 'y': 1640, 'w': 267, 'h': 375}, 'tags': [{'name': 'product', 'confidence': 0.9708327651023865}]}, {'id': '5', 'boundingBox': {'x': 587, 'y': 1642, 'w': 261, 'h': 370}, 'tags': [{'name': 'product', 'confidence': 0.970481276512146}]}, {'id': '6', 'boundingBox': {'x': 1145, 'y': 2152, 'w': 265, 'h': 386}, 'tags': [{'name': 'product', 'confidence': 0.9703117609024048}]}, {'id': '7', 'boundingBox': {'x': 852, 'y': 1639, 'w': 255, 'h': 370}, 'tags': [{'name': 'product', 'confidence': 0.9698545932769775}]}, {'id': '8', 'boundingBox': {'x': 359, 'y': 2160, 'w': 260, 'h': 381}, 'tags': [{'name': 'product', 'confidence': 0.9693546891212463}]}, {'id': '9', 'boundingBox': {'x': 621, 'y': 2160, 'w': 260, 'h': 380}, 'tags': [{'name': 'product', 'confidence': 0.9686022400856018}]}, {'id': '10', 'boundingBox': {'x': 1625, 'y': 1636, 'w': 262, 'h': 370}, 'tags': [{'name': 'product', 'confidence': 0.9685074687004089}]}, {'id': '11', 'boundingBox': {'x': 878, 'y': 646, 'w': 434, 'h': 369}, 'tags': [{'name': 'product', 'confidence': 0.9684135913848877}]}, {'id': '12', 'boundingBox': {'x': 1369, 'y': 1636, 'w': 255, 'h': 370}, 'tags': [{'name': 'product', 'confidence': 0.9675979614257812}]}, {'id': '13', 'boundingBox': {'x': 1890, 'y': 1628, 'w': 267, 'h': 378}, 'tags': [{'name': 'product', 'confidence': 0.9675582647323608}]}, {'id': '14', 'boundingBox': {'x': 1752, 'y': 637, 'w': 426, 'h': 375}, 'tags': [{'name': 'product', 'confidence': 0.9672552943229675}]}, {'id': '15', 'boundingBox': {'x': 886, 'y': 2157, 'w': 257, 'h': 379}, 'tags': [{'name': 'product', 'confidence': 0.9669260382652283}]}, {'id': '16', 'boundingBox': {'x': 15, 'y': 643, 'w': 428, 'h': 371}, 'tags': [{'name': 'product', 'confidence': 0.9663529992103577}]}, {'id': '17', 'boundingBox': {'x': 290, 'y': 2713, 'w': 262, 'h': 370}, 'tags': [{'name': 'product', 'confidence': 0.9660894274711609}]}, {'id': '18', 'boundingBox': {'x': 444, 'y': 642, 'w': 437, 'h': 373}, 'tags': [{'name': 'product', 'confidence': 0.9659835696220398}]}, {'id': '19', 'boundingBox': {'x': 1111, 'y': 1680, 'w': 254, 'h': 328}, 'tags': [{'name': 'product', 'confidence': 0.9655608534812927}]}, {'id': '20', 'boundingBox': {'x': 1312, 'y': 644, 'w': 431, 'h': 369}, 'tags': [{'name': 'product', 'confidence': 0.9653035402297974}]}, {'id': '21', 'boundingBox': {'x': 92, 'y': 2162, 'w': 264, 'h': 378}, 'tags': [{'name': 'product', 'confidence': 0.9652042388916016}]}, {'id': '22', 'boundingBox': {'x': 2167, 'y': 59, 'w': 417, 'h': 363}, 'tags': [{'name': 'product', 'confidence': 0.9651398062705994}]}, {'id': '23', 'boundingBox': {'x': 1674, 'y': 2154, 'w': 267, 'h': 384}, 'tags': [{'name': 'product', 'confidence': 0.9643312096595764}]}, {'id': '24', 'boundingBox': {'x': 1292, 'y': 54, 'w': 219, 'h': 375}, 'tags': [{'name': 'product', 'confidence': 0.9628825187683105}]}, {'id': '25', 'boundingBox': {'x': 0, 'y': 60, 'w': 410, 'h': 371}, 'tags': [{'name': 'product', 'confidence': 0.9624801874160767}]}, {'id': '26', 'boundingBox': {'x': 1937, 'y': 2154, 'w': 269, 'h': 385}, 'tags': [{'name': 'product', 'confidence': 0.962049663066864}]}, {'id': '27', 'boundingBox': {'x': 2206, 'y': 2158, 'w': 267, 'h': 381}, 'tags': [{'name': 'product', 'confidence': 0.9616059064865112}]}, {'id': '28', 'boundingBox': {'x': 2171, 'y': 644, 'w': 414, 'h': 364}, 'tags': [{'name': 'product', 'confidence': 0.9607388377189636}]}, {'id': '29', 'boundingBox': {'x': 842, 'y': 59, 'w': 222, 'h': 371}, 'tags': [{'name': 'product', 'confidence': 0.9600986838340759}]}, {'id': '30', 'boundingBox': {'x': 1879, 'y': 2710, 'w': 263, 'h': 373}, 'tags': [{'name': 'product', 'confidence': 0.9593764543533325}]}, {'id': '31', 'boundingBox': {'x': 1614, 'y': 2705, 'w': 263, 'h': 375}, 'tags': [{'name': 'product', 'confidence': 0.9577333927154541}]}, {'id': '32', 'boundingBox': {'x': 26, 'y': 2718, 'w': 261, 'h': 358}, 'tags': [{'name': 'product', 'confidence': 0.9574332237243652}]}, {'id': '33', 'boundingBox': {'x': 1065, 'y': 55, 'w': 222, 'h': 374}, 'tags': [{'name': 'product', 'confidence': 0.9558419585227966}]}, {'id': '34', 'boundingBox': {'x': 1092, 'y': 2699, 'w': 259, 'h': 381}, 'tags': [{'name': 'product', 'confidence': 0.9536007046699524}]}, {'id': '35', 'boundingBox': {'x': 830, 'y': 2710, 'w': 260, 'h': 369}, 'tags': [{'name': 'product', 'confidence': 0.9517773389816284}]}, {'id': '36', 'boundingBox': {'x': 1351, 'y': 2703, 'w': 262, 'h': 380}, 'tags': [{'name': 'product', 'confidence': 0.9510535597801208}]}, {'id': '37', 'boundingBox': {'x': 438, 'y': 1163, 'w': 246, 'h': 362}, 'tags': [{'name': 'product', 'confidence': 0.9505003094673157}]}, {'id': '38', 'boundingBox': {'x': 2142, 'y': 2719, 'w': 261, 'h': 359}, 'tags': [{'name': 'product', 'confidence': 0.948496401309967}]}, {'id': '39', 'boundingBox': {'x': 681, 'y': 1163, 'w': 248, 'h': 358}, 'tags': [{'name': 'product', 'confidence': 0.9424805045127869}]}, {'id': '40', 'boundingBox': {'x': 931, 'y': 1152, 'w': 238, 'h': 366}, 'tags': [{'name': 'product', 'confidence': 0.9419150352478027}]}, {'id': '41', 'boundingBox': {'x': 1646, 'y': 1145, 'w': 253, 'h': 372}, 'tags': [{'name': 'product', 'confidence': 0.9419048428535461}]}, {'id': '42', 'boundingBox': {'x': 1890, 'y': 1172, 'w': 259, 'h': 342}, 'tags': [{'name': 'product', 'confidence': 0.9412358999252319}]}, {'id': '43', 'boundingBox': {'x': 22, 'y': 546, 'w': 425, 'h': 95}, 'tags': [{'name': 'product', 'confidence': 0.940949559211731}]}, {'id': '44', 'boundingBox': {'x': 2161, 'y': 1628, 'w': 265, 'h': 381}, 'tags': [{'name': 'product', 'confidence': 0.9396277070045471}]}, {'id': '45', 'boundingBox': {'x': 1405, 'y': 1162, 'w': 247, 'h': 355}, 'tags': [{'name': 'product', 'confidence': 0.937685489654541}]}, {'id': '46', 'boundingBox': {'x': 2124, 'y': 1148, 'w': 277, 'h': 368}, 'tags': [{'name': 'product', 'confidence': 0.9362070560455322}]}, {'id': '47', 'boundingBox': {'x': 1169, 'y': 1151, 'w': 239, 'h': 365}, 'tags': [{'name': 'product', 'confidence': 0.9347727298736572}]}, {'id': '48', 'boundingBox': {'x': 1742, 'y': 543, 'w': 420, 'h': 93}, 'tags': [{'name': 'product', 'confidence': 0.9328020215034485}]}, {'id': '49', 'boundingBox': {'x': 2166, 'y': 534, 'w': 416, 'h': 102}, 'tags': [{'name': 'product', 'confidence': 0.9311664700508118}]}, {'id': '50', 'boundingBox': {'x': 111, 'y': 1614, 'w': 238, 'h': 365}, 'tags': [{'name': 'product', 'confidence': 0.9275888800621033}]}, {'id': '51', 'boundingBox': {'x': 1310, 'y': 539, 'w': 426, 'h': 99}, 'tags': [{'name': 'product', 'confidence': 0.9248339533805847}]}, {'id': '52', 'boundingBox': {'x': 454, 'y': 549, 'w': 426, 'h': 91}, 'tags': [{'name': 'product', 'confidence': 0.9220108389854431}]}, {'id': '54', 'boundingBox': {'x': 2398, 'y': 2690, 'w': 141, 'h': 381}, 'tags': [{'name': 'product', 'confidence': 0.8802869319915771}]}, {'id': '55', 'boundingBox': {'x': 885, 'y': 553, 'w': 422, 'h': 90}, 'tags': [{'name': 'product', 'confidence': 0.8378040790557861}]}, {'id': '57', 'boundingBox': {'x': 2430, 'y': 1632, 'w': 119, 'h': 373}, 'tags': [{'name': 'product', 'confidence': 0.7863535284996033}]}, {'id': '58', 'boundingBox': {'x': 663, 'y': 2606, 'w': 199, 'h': 189}, 'tags': [{'name': 'product', 'confidence': 0.5052989721298218}]}, {'id': '59', 'boundingBox': {'x': 2101, 'y': 2089, 'w': 258, 'h': 42}, 'tags': [{'name': 'product', 'confidence': 0.14365340769290924}]}], 'gaps': [{'id': '53', 'boundingBox': {'x': 59, 'y': 1081, 'w': 374, 'h': 448}, 'tags': [{'name': 'gap', 'confidence': 0.8853721618652344}]}, {'id': '56', 'boundingBox': {'x': 1518, 'y': 0, 'w': 203, 'h': 425}, 'tags': [{'name': 'gap', 'confidence': 0.8259734511375427}]}]}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from cognitive_service_vision_model_customization_python_samples.clients import ProductRecognitionClient\n",
    "from cognitive_service_vision_model_customization_python_samples.models import ProductRecognition\n",
    "\n",
    "client = ProductRecognitionClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "run_name = str(uuid.uuid4())\n",
    "model_name = 'ms-pretrained-product-detection'\n",
    "run = ProductRecognition(run_name, model_name)\n",
    "\n",
    "with open('./resources/sample_image.jpg', 'rb') as f:\n",
    "    img = f.read()\n",
    "\n",
    "try:\n",
    "    client.create_run(run, img, 'image/png')\n",
    "    client.wait_for_completion(run_name, model_name)\n",
    "finally:\n",
    "    client.delete_run(run_name, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run product recognition with customized product recognition model\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "Follow the [Cognitive Service Vision Model Customization document](https://github.com/Azure-Samples/cognitive-service-vision-model-customization-python-samples/blob/main/docs/cognitive_service_vision_model_customization.ipynb) to train a customized product recognition model\n",
    "\n",
    "Note that for product recognition model training, the dataset type needs to be **ObjectDetection**\n",
    "\n",
    "Once you finished training the model, replace the corresponding model_name in the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from cognitive_service_vision_model_customization_python_samples.clients import ProductRecognitionClient\n",
    "from cognitive_service_vision_model_customization_python_samples.models import ProductRecognition\n",
    "\n",
    "product_recognition_client = ProductRecognitionClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "run_name = str(uuid.uuid4())\n",
    "model_name = '{specify_your_customized_product_recognition_model_name}'\n",
    "run = ProductRecognition(run_name, model_name)\n",
    "\n",
    "with open('./resources/sample_image.jpg', 'rb') as f:\n",
    "    img = f.read()\n",
    "\n",
    "client.create_run(run, img, 'image/png')\n",
    "client.wait_for_completion(run_name, model_name)\n",
    "client.delete_run(run_name, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
