{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Cognitive Service Vision Product Recognition using Python\n",
    "\n",
    "This is a tutorial about using Cognitive Service Vision Product Recognition.\n",
    "\n",
    "Cognitive Service Vision Product Recognition can be used to detect and recognize packaged products.\n",
    "\n",
    "Currently, product recognition feature are available in **EastUS**, **West US2**, and **West Europe** regions.\n",
    "\n",
    "Please create a computer vision resource on Azure portal, in **EastUS**, **West US2**, or **West Europe** region, if you don't already have one. You can use [Multi-service resource](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Canomaly-detector%2Clanguage-service%2Ccomputer-vision%2Cwindows) as well. \n",
    "\n",
    "![check ../model_customization/media/create_cv_resource.png if pic does not show up](../model_customization/media/create_cv_resource.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the python samples package\n",
    "\n",
    "Install the sample code including utility code helping you use Python to run product recognition in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cognitive-service-vision-model-customization-python-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource and key\n",
    "import logging\n",
    "import uuid\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from cognitive_service_vision_model_customization_python_samples import ResourceType\n",
    "from cognitive_service_vision_model_customization_python_samples.clients import ProductRecognitionClient\n",
    "from cognitive_service_vision_model_customization_python_samples.models import ProductRecognition\n",
    "from cognitive_service_vision_model_customization_python_samples.tools import visualize_recognition_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "Resource name and resource key are needed for accessing the service, which you can find here:\n",
    "\n",
    "![check media/credentials.png if pic does not show up](./resources/credentials.png)\n",
    "\n",
    "If you are using a multi-service resource, the endpoint can be found here:\n",
    "\n",
    "![check resources/multi_service_endpoint.png if pic does not show up](./resources/multi_service_endpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource and key\n",
    "resource_type = ResourceType.SINGLE_SERVICE_RESOURCE # or ResourceType.MULTI_SERVICE_RESOURCE\n",
    "\n",
    "resource_name = None\n",
    "multi_service_endpoint = None\n",
    "\n",
    "if resource_type == ResourceType.SINGLE_SERVICE_RESOURCE:\n",
    "    resource_name = '{specify_your_resource_name}'\n",
    "    assert resource_name\n",
    "else:\n",
    "    multi_service_endpoint = '{specify_your_service_endpoint}'\n",
    "    assert multi_service_endpoint\n",
    "\n",
    "resource_key = '{specify_your_resource_key}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process your image using the image composition tool (Optional)\n",
    "\n",
    "Follow the tutorial [here](./cognitive_service_vision_image_composition.ipynb) to stitch and rectify your images.\n",
    "Stitching is needed when the shelf is too long or too high so that one image capture cannot cover the entire shelf.\n",
    "Rectification warps the image so that it looks frontal, it is required for doing planogram compliance. For product recognition, it's optional but recommended, as it will improve recognition quality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run product recognition with pre-built model\n",
    "\n",
    "You can run product recognition to detect products and gaps using our prebuilt model. The prebuilt model only detects whether an object is a product or a gap, it does not recognize different names of the products. You can use the prebuilt model to check scenarios like shelf occupancy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ProductRecognitionClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "run_name = str(uuid.uuid4())\n",
    "model_name = 'ms-pretrained-product-detection'\n",
    "run = ProductRecognition(run_name, model_name)\n",
    "\n",
    "with open('./resources/sample_image.jpg', 'rb') as f:\n",
    "    img = f.read()\n",
    "\n",
    "try:\n",
    "    client.create_run(run, img, 'image/png')\n",
    "    result = client.wait_for_completion(run_name, model_name)\n",
    "finally:\n",
    "    client.delete_run(run_name, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize product recognition result from pre-built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert img data to numpy array and decode numpy array to OpenCV BGR image\n",
    "cv_img = cv2.imdecode(np.frombuffer(img, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "# Visualize the result\n",
    "cv_img = visualize_recognition_result(result.result, cv_img)\n",
    "cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "display(Image.fromarray(cv_img))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run product recognition with customized product recognition model\n",
    "\n",
    "To get more detailed recognition result from the image, you will need to train a customized product recognition model. So that the model can recognize different types of product, using which inventory check, planogram compliance and other more detailed retail execution KPIs can be calculated.\n",
    "\n",
    "### Train customized product recognition model\n",
    "\n",
    "Follow the [Cognitive Service Vision Model Customization document](https://github.com/Azure-Samples/cognitive-service-vision-model-customization-python-samples/blob/main/docs/cognitive_service_vision_model_customization.ipynb) to train a customized product recognition model\n",
    "\n",
    "Note that for product recognition model training, the dataset type needs to be **ObjectDetection**, and the model kind needs to be **Product-Recognizer**\n",
    "\n",
    "Once you finished training the model, replace the corresponding model_name in the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ProductRecognitionClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "run_name = str(uuid.uuid4())\n",
    "model_name = '{specify_your_customized_product_recognition_model_name}'\n",
    "run = ProductRecognition(run_name, model_name)\n",
    "\n",
    "with open('./resources/sample_image.jpg', 'rb') as f:\n",
    "    img = f.read()\n",
    "\n",
    "try:\n",
    "    client.create_run(run, img, 'image/png')\n",
    "    result = client.wait_for_completion(run_name, model_name)\n",
    "finally:\n",
    "    client.delete_run(run_name, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize product recognition result from customized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert img data to numpy array and decode numpy array to OpenCV BGR image\n",
    "cv_img = cv2.imdecode(np.frombuffer(img, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "# Visualize the result\n",
    "cv_img = visualize_recognition_result(result.result, cv_img)\n",
    "cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "display(Image.fromarray(cv_img))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "With the accurate product recognition result, users can now effectively carry out various retail execution key performance indicators (KPIs). One essential KPI that can be measured is shelf occupancy, which involves monitoring the amount of shelf space occupied by specific products. By leveraging the product recognition outcome, retailers can analyze and optimize the distribution of products on the shelves, ensuring maximum visibility and availability.\n",
    "\n",
    "Additionally, for those who have a planogram in place, they can easily assess planogram compliance using the product recognition data, please refer to the tutorial [here](cognitive_service_vision_planogram_compliance.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
