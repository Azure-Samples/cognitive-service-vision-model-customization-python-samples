{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Cognitive Service Vision Model Customization using Python: Generic Image Classification and Object Detection\n",
    "\n",
    "This is a tutorial about using Cognitive Service Vision Model Customization to train your own image classifier (IC) or object detector (OD) with your own data using Python.\n",
    "\n",
    "Currently, both IC and OD are available in **EastUS**, **West US2**, and **West Europe** regions.\n",
    "\n",
    "Please create a computer vision resource on Azure portal, in **EastUS**, **West US2**, or **West Europe** region, if you don't already have one. You can use [Multi-service resource](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Canomaly-detector%2Clanguage-service%2Ccomputer-vision%2Cwindows) as well. \n",
    "\n",
    "![check media/create_cv_resource.png if pic does not show up](./media/create_cv_resource.png)\n",
    "\n",
    "## Credentials\n",
    "\n",
    "Resource name and resource key are needed for accessing the service, which you can find here:\n",
    "\n",
    "![check media/credentials.png if pic does not show up](./media/credentials.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource and key\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from cognitive_service_vision_model_customization_python_samples import ResourceType\n",
    "\n",
    "resource_type = ResourceType.SINGLE_SERVICE_RESOURCE # or ResourceType.MULTI_SERVICE_RESOURCE\n",
    "\n",
    "resource_name = None\n",
    "multi_service_endpoint = None\n",
    "\n",
    "if resource_type == ResourceType.SINGLE_SERVICE_RESOURCE:\n",
    "    resource_name = '{specify_your_resource_name}'\n",
    "    assert resource_name\n",
    "else:\n",
    "    multi_service_endpoint = '{specify_your_service_endpoint}'\n",
    "    assert multi_service_endpoint\n",
    "\n",
    "resource_key = '{specify_your_resource_key}'\n",
    "\n",
    "# Inputs\n",
    "model_name = '{specify_your_model_name}'\n",
    "dataset_name = '{specify_your_dataset_name}'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the python samples package\n",
    "\n",
    "Install the sample code including utility code helping you to train/predict with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pip install cognitive-service-vision-model-customization-python-samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare/register a dataset from Azure blob storage\n",
    "\n",
    "To train a model with your own dataset, the dataset should be prepared conformed with the coco format described below, hosted on Azure blob storage, and accessible from your Computer Vision resource.\n",
    "\n",
    "Quota limit information, including the max number of images and categories supported, max image size, etc, can be found at [quota_limit.md](./quota_limit.md).\n",
    "\n",
    "### Dataset annotation format\n",
    "\n",
    "We use coco format for indexing/organizing the images and their annotations. Below are the examples and explanations of what specific format is needed for multiclass classification and object detection.\n",
    "\n",
    "Note that\n",
    "\n",
    "- Cognitive service vision model customization for classification is different than other vision training, as we **utilize your class names as text information in training**, in addtion to your image data, so please provide meaningful category names in the annotations.\n",
    "- **Note that in our example dataset, there are only two images, just for the simplicity of the example**. Although [Florence models](https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/) running in the service achieves great few-shot performance (aka high model quality even with little data available), it is still great to have more data for the model to learn. Our recommendation is to have at least 5 images per class and the more the better.\n",
    "- **Once your coco annotation file is prepared, you can use [`check_coco_annotation.ipynb`](./check_coco_annotation.ipynb) in this repo to check your annotation file format.**\n",
    "\n",
    "#### Multiclass classification example\n",
    "\n",
    "```{json}\n",
    "{\n",
    "  \"images\": [{\"id\": 1, \"width\": 224.0, \"height\": 224.0, \"file_name\": \"images/siberian-kitten.jpg\", \"absolute_url\": \"https://{your_blob}.blob.core.windows.net/datasets/cat_dog/images/siberian-kitten.jpg\"},\n",
    "              {\"id\": 2, \"width\": 224.0, \"height\": 224.0, \"file_name\": \"images/kitten-3.jpg\", \"absolute_url\": \"https://{your_blob}.blob.core.windows.net/datasets/cat_dog/images/kitten-3.jpg\"}],\n",
    "  \"annotations\": [\n",
    "      {\"id\": 1, \"category_id\": 1, \"image_id\": 1},\n",
    "      {\"id\": 2, \"category_id\": 1, \"image_id\": 2},\n",
    "  ],\n",
    "  \"categories\": [{\"id\": 1, \"name\": \"cat\"}, {\"id\": 2, \"name\": \"dog\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "Besides `absolute_url`, you can also use `coco_url`, which works too (system accepts both field names, no difference.).\n",
    "\n",
    "#### Object detection example\n",
    "\n",
    "```{json}\n",
    "{\n",
    "  \"images\": [{\"id\": 1, \"width\": 224.0, \"height\": 224.0, \"file_name\": \"images/siberian-kitten.jpg\", \"absolute_url\": \"https://{your_blob}.blob.core.windows.net/datasets/cat_dog/images/siberian-kitten.jpg\"},\n",
    "              {\"id\": 2, \"width\": 224.0, \"height\": 224.0, \"file_name\": \"images/kitten-3.jpg\", \"absolute_url\": \"https://{your_blob}.blob.core.windows.net/datasets/cat_dog/images/kitten-3.jpg\"}],\n",
    "  \"annotations\": [\n",
    "      {\"id\": 1, \"category_id\": 1, \"image_id\": 1, \"bbox\": [0.1, 0.1, 0.3, 0.3]},\n",
    "      {\"id\": 2, \"category_id\": 1, \"image_id\": 2, \"bbox\": [0.3, 0.3, 0.6, 0.6]},\n",
    "      {\"id\": 3, \"category_id\": 2, \"image_id\": 2, \"bbox\": [0.2, 0.2, 0.7, 0.7]}\n",
    "  ],\n",
    "  \"categories\": [{\"id\": 1, \"name\": \"cat\"}, {\"id\": 2, \"name\": \"dog\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "`bbox: [left, top, width, height]` is expected to be numbers relative the image width and height.\n",
    "\n",
    "#### Blob/File directory structure\n",
    "\n",
    "Following the examples above, the data directory in your Azure Blob Container `https://{your_blob}.blob.core.windows.net/datasets/` should be like below, where `train_coco.json` is the annotation file, the format of which is described above.\n",
    "\n",
    "```\n",
    "cat_dog/\n",
    "    images/\n",
    "        1.jpg\n",
    "        2.jpg\n",
    "    train_coco.json\n",
    "```\n",
    "\n",
    "### Grant your Computer Vision resource access to your Azure data blob\n",
    "\n",
    "#### Option 1: Shared access signatures (SAS)\n",
    "\n",
    "You can generate SAS token with at least read permission on your Azure Blob Container for your data annotation and images, example usage of which can be found in the code below. For instructions of acquiring SAS token, check this [documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/how-to-guides/create-sas-tokens?tabs=Containers).\n",
    "\n",
    "#### Option 2: Managed identity or public accessible\n",
    "\n",
    "If your data is not public accessible, make sure to grant the Computer Vision resoure you created the access to your blob `https://{your_blob}.blob.core.windows.net/`, via [managed identiy](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview):\n",
    "\n",
    "Below is a standard practice for using system assigned managed identity of your created Computer Vision resource to access your blob storage, on Azure portal:\n",
    "\n",
    "- Once created the Computer Vision resource, go to the \"Identity / System assigned\" tab, change the \"Status\" to \"On\"\n",
    "- Go to your blob storage; go to \"Access Control (IAM) / Role assignment\", Click \"Add / Add role assignment\", choose \"Storage Blob Data Contributor\" or \"Storage Blob Data Reader\"\n",
    "- Click \"Next\", and choose \"Managed Identity\" for \"Assign access to\", and then click on \"Select members\"\n",
    "- Choose your subscription, Managed Identity being \"Computer Vision\", and look up the one that match your Computer Vision resource's name\n",
    "\n",
    "\n",
    "### Register dataset\n",
    "\n",
    "Once dataset has been properly prepared and hosted on your azure blob storage, with access granted to your Computer Vision resource, now let us register it in the service (**Data access only happens during training, service does not retain copies of your data beyond training cycle**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognitive_service_vision_model_customization_python_samples import DatasetClient, Dataset, AnnotationKind, AuthenticationKind, Authentication\n",
    "\n",
    "dataset_client = DatasetClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "\n",
    "auth_kind = AuthenticationKind.SAS # or AuthenticationKind.MI\n",
    "\n",
    "# register dataset\n",
    "if auth_kind == AuthenticationKind.SAS:\n",
    "   # option 1: sas\n",
    "   sas_auth = Authentication(AuthenticationKind.SAS, '{your_sas_token}') # note the token/query string is needed, not the full url\n",
    "   dataset = Dataset(name=dataset_name,\n",
    "                     annotation_kind=AnnotationKind.MULTICLASS_CLASSIFICATION,  # checkout AnnotationKind for all annotation kinds\n",
    "                     annotation_file_uris=['https://{your_blob}.blob.core.windows.net/datasets/cat_dog/train_coco.json'],\n",
    "                     authentication=sas_auth)\n",
    "else:\n",
    "   # option 2: managed identity or public accessible. make sure your storage is accessible via the managed identiy, if it is not public accessible\n",
    "   dataset = Dataset(name=dataset_name,\n",
    "                     annotation_kind=AnnotationKind.MULTICLASS_CLASSIFICATION,  # checkout AnnotationKind for all annotation kinds\n",
    "                     annotation_file_uris=['https://{your_blob}.blob.core.windows.net/datasets/cat_dog/train_coco.json'])\n",
    "\n",
    "reg_dataset = dataset_client.register_dataset(dataset)\n",
    "logging.info(f'Register dataset: {reg_dataset.__dict__}')\n",
    "\n",
    "# specify your evaluation dataset here\n",
    "eval_dataset = None\n",
    "if eval_dataset:\n",
    "   reg_eval_dataset = dataset_client.register_dataset(eval_dataset)\n",
    "   logging.info(f'Register eval dataset: {reg_eval_dataset.__dict__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "After registering a dataset, let us train a model with the registered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognitive_service_vision_model_customization_python_samples import TrainingClient, Model, ModelKind, TrainingParameters, EvaluationParameters\n",
    "\n",
    "training_client = TrainingClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "train_params = TrainingParameters(training_dataset_name=dataset_name, time_budget_in_hours=1, model_kind=ModelKind.GENERIC_IC)  # checkout ModelKind for all valid model kinds\n",
    "eval_params = EvaluationParameters(test_dataset_name=eval_dataset.name) if eval_dataset else None\n",
    "model = Model(model_name, train_params, eval_params)\n",
    "model = training_client.train_model(model)\n",
    "logging.info(f'Start training: {model.__dict__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognitive_service_vision_model_customization_python_samples import TrainingClient\n",
    "\n",
    "training_client = TrainingClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "model = training_client.wait_for_completion(model_name, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with a sample image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognitive_service_vision_model_customization_python_samples import PredictionClient\n",
    "prediction_client = PredictionClient(resource_type, resource_name, multi_service_endpoint, resource_key)\n",
    "\n",
    "with open('./media/microsoft_logo.png', 'rb') as f:\n",
    "    img = f.read()\n",
    "\n",
    "prediction = prediction_client.predict(model_name, img, content_type='image/png')\n",
    "logging.info(f'Prediction: {prediction}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2790c31357c8ba2c3943b53ce111c55730e38f0e9239632a8827c0596114be0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
